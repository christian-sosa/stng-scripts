{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "import boto3\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "#from weeklyIncrement_ import concat_list, weekly_increment\n",
    "#from weeklyDataIncrementSync import df_loader, esn_list, weekly_increment_sync\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "s3resource = boto3.resource('s3')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "os version:  3.8.6 (v3.8.6:db455296be, Sep 23 2020, 13:31:39) \n",
      "[Clang 6.0 (clang-600.0.57)]\n",
      "numpy version:  1.19.4\n",
      "pandas version:  1.3.0\n",
      "matplotlib version:  3.3.3\n",
      "tensorflow version:  2.7.0\n",
      "tensorflow-probability version:  0.15.0\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"os version: \", os.sys.version)\n",
    "print(\"numpy version: \", np.__version__)\n",
    "print(\"pandas version: \", pd.__version__)\n",
    "print(\"matplotlib version: \", mpl.__version__)\n",
    "print(\"tensorflow version: \", tf.__version__)\n",
    "print(\"tensorflow-probability version: \", tfp.__version__)\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_loader():\n",
    "    ## recibe un esn y un path\n",
    "    ## y devuelve una lista de los pathnames que coincidan con el path que armo con esos datos\n",
    "    folders = []\n",
    "    my_bucket = s3resource.Bucket('stnglambdaoutput')\n",
    "    folder = getFolder()\n",
    "    folder = 'Paso4/' + folder\n",
    "    i = 0\n",
    "    for object_summary in my_bucket.objects.filter(Prefix=folder):\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "        else:\n",
    "            folders.append(object_summary.key)\n",
    "    print(folders)\n",
    "    return folders\n",
    "\n",
    "def getFolder():\n",
    "    HOY=dt.today()\n",
    "    dia = HOY - timedelta(days=2)\n",
    "    dia = dia.strftime('%A')\n",
    "  \n",
    "    dayOfWeek = [\"Friday\", \"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"]\n",
    "    i=0\n",
    "    for x in dayOfWeek:\n",
    "        if dia == x:\n",
    "            aux = i\n",
    "        i += 1\n",
    "        \n",
    "    if aux == 0:\n",
    "        folder = HOY - timedelta(days=2)\n",
    "        folder = folder.strftime('%Y%m%d')\n",
    "        folder = folder + '/'\n",
    "    else:\n",
    "        count = 2 + aux\n",
    "        folder = HOY - timedelta(days=count)\n",
    "        folder = folder.strftime('%Y%m%d')\n",
    "        folder = folder + '/'\n",
    "    return folder\n",
    "def data_frame(data, print_info=False):\n",
    "    kwargs = {'delimiter': ',',\n",
    "              \"thousands\": ',',\n",
    "              \"header\": 0,\n",
    "              'na_values': np.nan,\n",
    "              'parse_dates': [\"devicetimestamp\"],\n",
    "              \"index_col\": \"devicetimestamp\"}\n",
    "    dff = pd.read_csv('s3://stnglambdaoutput/' +data[0], **kwargs)\n",
    "    dff.interpolate(inplace=True)\n",
    "    for i in range(1, len(data)):\n",
    "        df_ = pd.read_csv('s3://stnglambdaoutput/' +data[i], **kwargs)\n",
    "        df_.interpolate(inplace=True)\n",
    "        dff = dff.append(df_)\n",
    "        \n",
    "    if print_info:\n",
    "        print(dff.info())\n",
    "\n",
    "    return dff\n",
    "\n",
    "\n",
    "def kalman_filter(data_, xhat, P, xhatminus, Pminus, K, R, sz: int):\n",
    "    # initial parameters\n",
    "    z = data_  # observations\n",
    "    Q = 1e-5  # process variance\n",
    "\n",
    "    # Initial guess\n",
    "    xhat[0] = data_[0]\n",
    "    #P[0] = 1.0\n",
    "\n",
    "    for k in range(1, sz):\n",
    "        # time update\n",
    "        xhatminus[k] = xhat[k - 1]\n",
    "        Pminus[k] = P[k - 1] + Q\n",
    "        # measurement update\n",
    "        K[k] = Pminus[k] / (Pminus[k] + R)\n",
    "        xhat[k] = xhatminus[k] + K[k] * (z[k] - xhatminus[k])\n",
    "        P[k] = (1 - K[k]) * Pminus[k]\n",
    "\n",
    "    return xhat \n",
    "        \n",
    "    \n",
    "def kalman_viz(data,col):\n",
    "    plt.plot(data, 'k+', label='noisy measurement')\n",
    "    plt.plot(xhat, 'b-', label='a posteri estimate')\n",
    "    plt.legend()\n",
    "    plt.title(col)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Measurement')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def classCounter(df_f):\n",
    "    tbo_count = 0\n",
    "    pl_count = 0\n",
    "    cs_count = 0\n",
    "    ind_count = 0\n",
    "    for esn in df_f.index:\n",
    "        if df_f.loc[esn,\"TBO\"] >= 0.5:\n",
    "            tbo_count += 1\n",
    "        elif df_f.loc[esn, \"Seized_PL\"] >= 0.5:\n",
    "            pl_count += 1\n",
    "        elif df_f.loc[esn, \"Seized_CkL\"] >= 0.5:\n",
    "            cs_count +=1\n",
    "        else:\n",
    "            ind_count += 1\n",
    "    print(f\"TBOs: {tbo_count}, Vent: {pl_count}, Cronn rod seized: {cs_count}, IND: {ind_count}\")\n",
    "    \n",
    "    plt.bar(\"TBO\", height=tbo_count, alpha=.7)\n",
    "    plt.bar(\"Vent\", height=pl_count, alpha=.7)\n",
    "    plt.bar(\"Cronn rod seized\", height=cs_count, alpha=.7, edgecolor='b')\n",
    "    plt.bar(\"Indeterminated\", height=ind_count, alpha=.7, edgecolor='b')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def classID(df_f):        \n",
    "        \n",
    "    for esn in df_f.index:\n",
    "        df_f.loc[esn,'op_h'] = dff[dff.esn==esn].loc[:,'engine_op_h'].max()\n",
    "        \n",
    "        if df_f.loc[esn,\"TBO\"] >= 0.5:\n",
    "            df_f.loc[esn,\"Failure_mode\"] = \"TBO/SUSP\"\n",
    "        elif df_f.loc[esn, \"Seized_PL\"] >= 0.5:\n",
    "            df_f.loc[esn,\"Failure_mode\"] = \"VENTILADO\"\n",
    "        elif df_f.loc[esn, \"Seized_CkL\"] >= 0.5:\n",
    "            df_f.loc[esn,\"Failure_mode\"] = \"FUNDIDO\"\n",
    "        else:\n",
    "            df_f.loc[esn,\"Failure_mode\"] = \"IND\"\n",
    "            \n",
    "            \n",
    "    return df_f\n",
    "\n",
    "\n",
    "def create_dataset(X, y, time_step=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_step):\n",
    "        v = X.iloc[i:(i+time_step)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[(i+time_step)])\n",
    "        \n",
    "    return np.array(Xs), np.array(ys) \n",
    "\n",
    "\n",
    "def hours_aggregation(df_f, dff):\n",
    "    for esn in df_f.index:\n",
    "        df_f.loc[esn, 'op_h'] = dff[dff.esn == esn].loc[:, 'engine_op_h'].max()\n",
    "\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(ytrue, ypred):\n",
    "    return -ypred.log_prob(ytrue)\n",
    "\n",
    "\n",
    "divergence_fn = lambda q,p,_ : tfd.kl_divergence(q,p)/19930\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def nll(label_true, label_pred):\n",
    "    return -label_pred.log_prob(label_true)\n",
    "\n",
    "\n",
    "def class_model():\n",
    "    modelj = tf.keras.Sequential()\n",
    "    modelj.add(tfkl.InputLayer(input_shape=(30, 15)))\n",
    "\n",
    "    modelj.add(tfkl.Conv1D(filters=32, kernel_size=58,\n",
    "                           padding='causal', strides=1, dilation_rate=1\n",
    "                           ))\n",
    "    modelj.add(tfkl.BatchNormalization())\n",
    "    modelj.add(tfkl.ReLU())\n",
    "    modelj.add(\n",
    "        tfkl.MaxPool1D())\n",
    "    modelj.add(\n",
    "        tfpl.Convolution1DFlipout(\n",
    "            filters=8,\n",
    "            kernel_size=2,\n",
    "            padding='SAME',\n",
    "            activation=tf.nn.sigmoid,\n",
    "            strides=1,\n",
    "            # dilation_rate=1,\n",
    "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
    "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "            kernel_divergence_fn=divergence_fn,\n",
    "            bias_prior_fn=tfpl.default_multivariate_normal_fn,\n",
    "            bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "            bias_divergence_fn=divergence_fn\n",
    "        ))\n",
    "    modelj.add(\n",
    "        tfkl.MaxPool1D())\n",
    "    modelj.add(\n",
    "        tfkl.Flatten())\n",
    "\n",
    "    modelj.add(tfkl.BatchNormalization())\n",
    "\n",
    "    modelj.add(tfpl.DenseFlipout(\n",
    "        units=128,\n",
    "        activation=tf.nn.sigmoid,  # \"linear\",\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        bias_divergence_fn=divergence_fn,\n",
    "        bias_prior_fn=tfpl.default_multivariate_normal_fn,\n",
    "        bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False))\n",
    "    )\n",
    "\n",
    "    modelj.add(\n",
    "        tfpl.DenseFlipout(\n",
    "            units=tfpl.OneHotCategorical.params_size(3), activation=None,\n",
    "            kernel_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "            kernel_divergence_fn=divergence_fn,\n",
    "            bias_prior_fn=tfpl.default_multivariate_normal_fn,\n",
    "            bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "            bias_divergence_fn=divergence_fn\n",
    "        ))\n",
    "    modelj.add(\n",
    "        tfpl.OneHotCategorical(3)\n",
    "    )\n",
    "\n",
    "    modelj.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.8),\n",
    "        loss=nll,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    weightsj = 's3://stnglambdainput/modele_weights_30_accj_RT_e.h5'\n",
    "    modelj.load_weights(weightsj)\n",
    "\n",
    "    return modelj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(data_):\n",
    "    a = []\n",
    "    model = class_model()\n",
    "\n",
    "    assert type(data_) == list, \"data argument is not a list. It must be a list.\"\n",
    "\n",
    "    for i in range(len(data_)):\n",
    "        kwargs = {'delimiter': ',',\n",
    "                  \"thousands\": ',',\n",
    "                  \"header\": 0,\n",
    "                  'na_values': np.nan,\n",
    "                  'parse_dates': [\"devicetimestamp\"],\n",
    "                  \"index_col\": \"devicetimestamp\"}\n",
    "        df = pd.read_csv('s3://stnglambdaoutput/'+data_[i], **kwargs)\n",
    "        df = df.loc[:].apply(lambda col: np.where(col.isnull(), np.mean(col), col))\n",
    "        df.interpolate(inplace=True)\n",
    "        df = df.applymap(lambda x: 0 if x < 0 else x)\n",
    "        df.interpolate(inplace=True)\n",
    "\n",
    "        esn_df = df['esn']\n",
    "        df = df.drop(['esn'], axis=1)\n",
    "\n",
    "        if df.shape[0] > 30.0:\n",
    "\n",
    "            data = {}\n",
    "            for col in df.columns[:-2]:\n",
    "                sz: int\n",
    "                sz = len(list(df.index))\n",
    "                # Allocate space for arrays\n",
    "                xhat = np.zeros(sz)  # a posteri estimate of x\n",
    "                P = np.zeros(sz)  # a posteri error estimate\n",
    "                P[0] = 1.0\n",
    "                xhatminus = np.zeros(sz)  # a priori estimate of x\n",
    "                Pminus = np.zeros(sz)  # a priori error estimate\n",
    "                K = np.zeros(sz)  # gain or blending factor\n",
    "                R = 1.0 ** 3  # estimate of measurement variance, chanf¡ge to see effects\n",
    "                xhat = kalman_filter(df[col].values, xhat, P, xhatminus, Pminus, K, R, sz)\n",
    "                # kalman_viz(df[col].values, col)\n",
    "                data[col] = xhat\n",
    "\n",
    "            df_ = pd.DataFrame(data, index=df.index[0:], columns=df.columns[:-2])\n",
    "            df_[\"op_h\"] = df.engine_op_h\n",
    "            df_[\"rul\"] = df.rul\n",
    "            df_[\"esn\"] = esn_df.values[0:]\n",
    "\n",
    "            df_ = df_.set_index(['esn', df.index])\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            x_cuop_std = pd.DataFrame(scaler.fit_transform(df_), columns=df_.columns, index=df_.index, dtype='float16')\n",
    "\n",
    "            tf.random.set_seed(1234)\n",
    "\n",
    "            for esn in list(esn_df.unique()):\n",
    "                x_cuop_array, y_true_array = create_dataset(x_cuop_std.loc[esn], x_cuop_std.loc[esn, \"egt\"], time_step=30)\n",
    "                a.append(np.sum(model.predict(x_cuop_array), axis=0) / x_cuop_array.shape[0])\n",
    "                continue\n",
    "\n",
    "    df_f = pd.DataFrame(np.asarray(a), index=dff.esn.unique(),\n",
    "                        columns=[\"TBO\", \"Seized_PL\", \"Seized_CkL\"])\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderDate = getFolder()\n",
    "    \n",
    "path_inc_sync = 's3://stnglambdaoutput/Paso4/' + folderDate  \n",
    "data1 = file_loader()\n",
    "data1.sort()\n",
    "\n",
    "dff = data_frame(data1)\n",
    "df_f = classifier(data_=data1)\n",
    "    #classCounter(df_f)\n",
    "df_f = classID(df_f)\n",
    "df_f = hours_aggregation(df_f,  dff)\n",
    "\n",
    "nombreBucketDestino = 'stnglambdaoutput'\n",
    "folder2 = 'Paso5/'\n",
    "s3.put_object(Bucket=nombreBucketDestino, Key=folder2)\n",
    "\n",
    "pathForReport = 's3://stnglambdaoutput/Paso5/'+folderDate\n",
    "\n",
    "df_f.to_csv(pathForReport + \".csv\", index_label=\"ESN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
